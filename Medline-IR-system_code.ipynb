{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jenish Dobariya\n",
    "\n",
    "#### Date: 9th Febuary 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading inverted index (H) and computing DocLen (DL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # terms: 11463\n",
      " - Entry for 'pentobarbit': df=4, idf=2.412040330191658\n",
      " - Entry for 'defici': df=39, idf=1.4230357144931214\n",
      " - Entry for 'treatment': df=172, idf=0.7785718746120717\n",
      "\n",
      "Total # documents: 1033\n",
      " - Vector len for Doc 59 = 13.811725366348801\n",
      " - Vector len for Doc 1033 = 31.163653356034512\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "\n",
    "tindexfile = 'medline_term_index.csv'\n",
    "invindexfile = 'medline_inverted_index.csv'\n",
    "dindexfile = 'medline_doc_index.csv'\n",
    "\n",
    "# Number of documents in the corpus (hard-coded for this corpus)\n",
    "N = 1033\n",
    "\n",
    "# Major data structures\n",
    "H_invindex = {} # inverted index; term -> (idf, L:hashmap of (docID . tf))\n",
    "DL_doclen = {}  # document lengths; docID -> len\n",
    "\n",
    "## (1) Read the term index file and populate the invindex first\n",
    "tid2term_map = {} # temporary storage to hold mappings of termID -> term\n",
    "\n",
    "fin = open(tindexfile, 'r', encoding='utf-8')\n",
    "reader = csv.reader(fin, delimiter='\\t')\n",
    "for line in reader:\n",
    "    term = line[0]    # term string\n",
    "    termID = line[1]  # termID\n",
    "    df = int(line[2]) # document frequency\n",
    "    idf = math.log10(N/df) # idf\n",
    "    # record term -> (idf, emptyL) in H\n",
    "    H_invindex[term] = (idf, dict())\n",
    "    # record termID -> term \n",
    "    tid2term_map[termID] = term \n",
    "fin.close()\n",
    "\n",
    "## (2) Read the inverted index file and add postings lists in H.\n",
    "## Also compute document lengths too, incrementally -- and record in DL.\n",
    "fin = open(invindexfile, 'r')\n",
    "reader = csv.reader(fin, delimiter='\\t')\n",
    "for line in reader:\n",
    "    termID = line[0]\n",
    "    idx = 1\n",
    "    while idx < (len(line)-1):\n",
    "        docID = line[idx]\n",
    "        tf = int(line[idx+1]) # raw tf of the term in this document\n",
    "        # Record docID -> tf in term's L\n",
    "        L = (H_invindex[tid2term_map[termID]])[1]\n",
    "        L[docID] = tf  # docID -> raw term frequency\n",
    "        \n",
    "        # Accumulate the component vector length for the document\n",
    "        tfidf = tf * (H_invindex[tid2term_map[termID]])[0] # tf * idf\n",
    "        tfidfsq = math.pow(tfidf, 2.0)\n",
    "        if docID in DL_doclen:\n",
    "            DL_doclen[docID] += tfidfsq\n",
    "        else:\n",
    "            DL_doclen[docID] = tfidfsq\n",
    "        #\n",
    "        idx += 2\n",
    "fin.close()\n",
    "\n",
    "# Fix the DL entries by applying sqrt to make vector length.\n",
    "for docID in DL_doclen.keys():\n",
    "    val = DL_doclen[docID]\n",
    "    DL_doclen[docID] = math.sqrt(val)\n",
    "\n",
    "    \n",
    "print ('Total # terms: %d' % len(H_invindex))\n",
    "for term in ['pentobarbit', 'defici', 'treatment']:\n",
    "    print (' - Entry for \\'%s\\': df=%s, idf=%s' % (term, len(H_invindex[term][1]), H_invindex[term][0]))\n",
    "\n",
    "print ('\\nTotal # documents: %d' % len(DL_doclen))\n",
    "for docID in ['59', '1033']:\n",
    "    print (' - Vector len for Doc %s = %s' % (docID, DL_doclen[docID]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries as Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # queries: 30\n",
      " - Query 2: {'relationship': 1, 'blood': 1, 'cerebrospin': 1, 'fluid': 1, 'oxygen': 1, 'concentr': 1, 'partial': 1, 'pressur': 1, 'method': 1, 'interest': 1, 'polarographi': 1}\n",
      " - Query 22: {'mycoplasma': 1, 'infect': 1, 'presenc': 1, 'embryo': 1, 'fetu': 1, 'newborn': 1, 'infant': 1, 'anim': 1, 'pregnanc': 1, 'gynecolog': 1, 'diseas': 1, 'relat': 1, 'chromosom': 2, 'abnorm': 1}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "queryfile = 'medline.query'\n",
    "\n",
    "# A list of queries. Each Query is a tuple, (qID, Q:term->tf map)\n",
    "Queries_list = []\n",
    "\n",
    "fin = open(queryfile, 'r', encoding='utf-8')#'iso-8859-1')\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "for line in fin:\n",
    "    matchObj = re.match(r'^(\\d+)\\s+(.*)', line)\n",
    "    if not matchObj:\n",
    "        print (\"ERROR with line -- %s\" % line)\n",
    "    else:\n",
    "        queryID = matchObj.group(1) # queryID\n",
    "        text = matchObj.group(2)    # query string (ignoring sentences)\n",
    "\n",
    "        # process text string -- same processing as one applied to documents.\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        terms = [porter.stem(w) for w in tokens if w not in stopwords.words('english') and len(w) > 1] # (**)\n",
    "        # term frequencies of the terms in this query are obtained by NLTK's FreqDist\n",
    "        fdist = nltk.FreqDist(terms)\n",
    "        # append the Query in the list\n",
    "        Queries_list.append((queryID, dict(fdist)))\n",
    "fin.close()\n",
    "\n",
    "print ('Total # queries: %d' % len(Queries_list))\n",
    "for qid in [1, 21]:\n",
    "    print (' - Query %s: %s' % (Queries_list[qid][0], Queries_list[qid][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval and Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing numpy for computational purposes\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_query_vector(query_terms, H_invindex):\n",
    "    # Getting a query vector the size of H_invindex\n",
    "    query_vector = np.zeros(len(H_invindex))\n",
    "    \n",
    "    # Iterating through the terms and tuples in inverted index\n",
    "    for term_idx, (term, (idf,_)) in enumerate(H_invindex.items()):\n",
    "        if term in query_terms:\n",
    "            tf = query_terms[term] # Creating TF value\n",
    "            \n",
    "             # getting the query vector value with tf_iDF\n",
    "            query_vector[term_idx] = tf * idf\n",
    "    return query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating a list to store all query vector\n",
    "query_vectors = []\n",
    "\n",
    "# Iterating though the Queries_list created before \n",
    "for query_id, query_terms in Queries_list:\n",
    "    # Passing the query_term and inverted index to get the query_vector\n",
    "    query_vector = compute_query_vector(query_terms, H_invindex)\n",
    "    \n",
    "    # storing the new found vector to the list\n",
    "    query_vectors.append((query_id, query_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing an empty dictionary to store document vectors\n",
    "document_vectors = {}\n",
    "\n",
    "# Iterating through DL_doclen\n",
    "for doc_id, doc_vector in DL_doclen.items():\n",
    "    # Initializing zero vectors as the length of inverted index\n",
    "    document_vector = np.zeros(len(H_invindex))\n",
    "    \n",
    "    # Iterating though inverted index\n",
    "    for term_idx, (term, (idf, _)) in enumerate(H_invindex.items()):\n",
    "        if doc_id in H_invindex[term][1]:\n",
    "            # getting tf value\n",
    "            tf = H_invindex[term][1][doc_id]\n",
    "            \n",
    "            # calculating the document vector\n",
    "            document_vector[term_idx] = tf * idf\n",
    "    document_vectors[doc_id] = document_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(query_vector, doc_vector):\n",
    "    # Computing the dot product using numpy between query and doc vector\n",
    "    dot_product = np.dot(query_vector, doc_vector)\n",
    "    # Computing L2 norm of the query and document vector\n",
    "    query_norm = np.linalg.norm(query_vector)\n",
    "    doc_norm = np.linalg.norm(doc_vector)\n",
    "    \n",
    "    # If both are non zero compute the cosine similarity \n",
    "    if query_norm != 0 and doc_norm != 0:\n",
    "        return dot_product / (query_norm * doc_norm)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing empty list to store the ranked list\n",
    "ranked_lists = []\n",
    "\n",
    "# Itereating through the query vectors\n",
    "for query_id, query_vector in query_vectors:\n",
    "    similarities = [] # Initializing an empty list to store the document similarities\n",
    "    \n",
    "    # Iterating through the document vectors to find cosine similarity\n",
    "    for doc_id, doc_vector in document_vectors.items():\n",
    "        if cosine_similarity(query_vector, doc_vector) != 0:\n",
    "            similarities.append((doc_id, cosine_similarity(query_vector, doc_vector)))\n",
    "    \n",
    "    # Sorting the list of similarities based on similarity scores in decending order\n",
    "    ranked_list = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    ranked_lists.append((query_id, ranked_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing an empty dictionary to store query results\n",
    "query_results = {}\n",
    "\n",
    "# Iterating through ranked_lists\n",
    "for query_ids, ranked_list in ranked_lists:\n",
    "    ranked_document_ids = [] # Initializing an empty list to store docIDs\n",
    "    \n",
    "    # Iterating through ranked doc in the ranked list\n",
    "    for rank, (doc_ids, similarity) in enumerate(ranked_list):\n",
    "        ranked_document_ids.append(doc_ids)\n",
    "        \n",
    "    # Store the list of ranked document ids in the query results dictionary\n",
    "    query_results[query_ids] = ranked_document_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to a file\n",
    "with open(\"rankedlist.txt\", \"w\") as fin:\n",
    "    # Iterating through query_results dictionary\n",
    "    for query_id, document_ids in query_results.items():\n",
    "        # Joining all doc_ids separated by comma\n",
    "        document_ids_line = \", \".join(document_ids)\n",
    "        # Writing to the file\n",
    "        fin.write(f\"{query_id}, {document_ids_line}\\n\")\n",
    "        \n",
    "fin.close() # Closing the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation -- Computing Mean Average Precision (MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_answers_file(file_path):\n",
    "    answers = {} # Creating an empty dictionary to store doc_id and key values\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\" \") # Separating all values in a line\n",
    "            query_id = parts[0] # Getting queryid at index 0\n",
    "            relevant_docs = parts[1:] # getting relevent docids\n",
    "            \n",
    "            # Appending the queryid and docids to answer\n",
    "            answers[query_id] = relevant_docs\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = read_answers_file('medline.rel')\n",
    "\n",
    "# storing the queryid and docids provided by the algorithm\n",
    "my_res = {}\n",
    "\n",
    "# Iterating through Ranked_list to get queryid and docid\n",
    "for query_id, ranked_list in ranked_lists:\n",
    "    my_docs = [doc_ids for doc_ids, _ in ranked_list]\n",
    "    my_res[query_id] = my_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_precision(ranked_dic, eval_dic):\n",
    "        average_precision_dic = {} # Creating empty dictionary\n",
    "        # Iterating through the ranked_dict\n",
    "        for q,dID in ranked_dic.items():\n",
    "            precisionk = 0.0\n",
    "            relevant_cnt = 0\n",
    "            # Iterating through the document ID from ranked_dictionary\n",
    "            for k, docID in enumerate(dID, start=1):\n",
    "                \n",
    "                # Checking if the docID is in the evaluation dictionary\n",
    "                if docID in eval_dic[q]:\n",
    "                    relevant_cnt +=1\n",
    "                    precisionk += relevant_cnt / k\n",
    "            \n",
    "            # Calculating average precision\n",
    "            average_precision = precisionk / relevant_cnt\n",
    "            average_precision_dic[q] = average_precision\n",
    "            \n",
    "        return average_precision_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = calculate_avg_precision(my_res, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.5656605218653455\n"
     ]
    }
   ],
   "source": [
    "MAP = 0.0\n",
    "\n",
    "# Iterating through the dictionary to sum the average precision\n",
    "for i,j in out.items():\n",
    "    MAP+=j\n",
    "\n",
    "# Getting the Mean Average Precision\n",
    "print(f\"MAP: {MAP/30}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
